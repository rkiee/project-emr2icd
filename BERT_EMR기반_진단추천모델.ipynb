{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPBS86Jg/uczdRgOwH9isjL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkiee/project-emr2icd/blob/main/BERT_EMR%EA%B8%B0%EB%B0%98_%EC%A7%84%EB%8B%A8%EC%B6%94%EC%B2%9C%EB%AA%A8%EB%8D%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "maG93-lmxLF-"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "7V1aSw5m2w9o"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/신장내과_본원_외래_20230701_20231231_filtered.xlsx'\n",
        "\n",
        "selected_columns = ['ICD10', 'transstr']\n",
        "df = pd.read_excel(file_path, usecols=selected_columns)\n",
        "\n",
        "print('전체 데이터 수 :',len(df))\n",
        "# df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGjdysN041BS",
        "outputId": "0819f0cc-ace4-4b3e-c721-95e846f8c8ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 데이터 수 : 826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습용, 테스트용, 검증용 데이터 분할\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(df['transstr'], df['ICD10'], test_size=0.2, random_state=5)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=7)\n",
        "\n",
        "# print(X_train)\n",
        "# print(\"=======================================\\n\")\n",
        "# print(X_test)\n",
        "# print(\"=======================================\\n\")\n",
        "# print(X_val)\n",
        "# print(\"=======================================\\n\")\n",
        "# print(y_train)\n",
        "# print(\"=======================================\\n\")\n",
        "# print(y_test)\n",
        "# print(\"=======================================\\n\")\n",
        "# print(y_val)"
      ],
      "metadata": {
        "id": "SxMXKPNg5Jx_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://zzaebok.github.io/deep_learning/nlp/Bert-for-classification/"
      ],
      "metadata": {
        "id": "XBi6fikpk2H9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# device = torch.device(device_name)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('NitzanBar/umls-spanbert')\n",
        "\n",
        "# Model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('NitzanBar/umls-spanbert')\n",
        "# model.to(device)"
      ],
      "metadata": {
        "id": "SYy4tzpDMdXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwwpIVbRfA_b",
        "outputId": "d6705fdd-aa98-40eb-f2e7-168c26613bae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "443    N185\n",
            "441    N183\n",
            "66     R311\n",
            "773    N183\n",
            "131    N183\n",
            "785    R601\n",
            "356    N184\n",
            "208    N178\n",
            "346    N391\n",
            "540    N184\n",
            "296    R601\n",
            "614    N185\n",
            "306    N182\n",
            "279    N183\n",
            "266    N028\n",
            "153    Z524\n",
            "163    Z524\n",
            "353    R311\n",
            "45     Z524\n",
            "366    N391\n",
            "Name: ICD10, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# label encoding\n",
        "unique_label = y_train.unique() # 중복없는 고유한 값 추출\n",
        "print(\"1 :\", unique_label)\n",
        "\n",
        "label_encoder = LabelEncoder() # 데이터를 숫자 형태로 인코딩할 수 있도록 함\n",
        "label_encoder.fit(unique_label)\n",
        "# LabelEncoder 객체가 실제 데이터에 맞게 학습 (레이블과 해당하는 숫자 값 간의 매핑을 학습)\n",
        "# LabelEncoder 객체가 학습된 후에는 어떤 레이블이 주어져도 그에 해당하는 숫자 값으로 변환할 수 있다\n",
        "\n",
        "# print(\"2 : \", label_encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "QEhgUs4FAVnH",
        "outputId": "52959707-d2ae-48bf-afed-e9ac5b9b660e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : ['N185' 'N183' 'R311' 'R601' 'N184' 'N178' 'N391' 'N182' 'N028' 'Z524'\n",
            " 'R808' 'N029' 'N059' 'R798']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df = y_train.to_frame()\n",
        "temp_df['encoded_ICD10'] = label_encoder.transform(temp_df['ICD10']) # 숫자로 변환 : 머신 러닝 모델은 보통 숫자를 입력으로 사용하므로\n",
        "y_train_encoding = pd.Series(temp_df['encoded_ICD10'])\n",
        "# Y_train_encoding = Y_train_encoding.rename('encoded_ICD10')\n",
        "print(\"2 :\", y_train_encoding[:20])\n",
        "\n",
        "# import numpy as np\n",
        "# min_value = np.min(y_train_encoding)\n",
        "# max_value = np.max(y_train_encoding)\n",
        "# print(\"3 :\", min_value, max_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1kC16MbBD2W",
        "outputId": "7e55540c-05db-4d22-f78f-bf3096bd145b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 : 443     7\n",
            "441     5\n",
            "66      9\n",
            "773     5\n",
            "131     5\n",
            "785    10\n",
            "356     6\n",
            "208     3\n",
            "346     8\n",
            "540     6\n",
            "296    10\n",
            "614     7\n",
            "306     4\n",
            "279     5\n",
            "266     0\n",
            "153    13\n",
            "163    13\n",
            "353     9\n",
            "45     13\n",
            "366     8\n",
            "Name: encoded_ICD10, dtype: int64\n",
            "3 : 0 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(temp_df)"
      ],
      "metadata": {
        "id": "p__zcmZaeZQL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5276a460-2ca6-47b2-aee9-a07021e1559b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    ICD10  encoded_ICD10\n",
            "443  N185              7\n",
            "441  N183              5\n",
            "66   R311              9\n",
            "773  N183              5\n",
            "131  N183              5\n",
            "..    ...            ...\n",
            "73   Z524             13\n",
            "400  R311              9\n",
            "118  N183              5\n",
            "701  N184              6\n",
            "206  N183              5\n",
            "\n",
            "[660 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(y_train_encoding[:20])\n",
        "print(y_train_encoding)"
      ],
      "metadata": {
        "id": "_Z0LdrSsh6F2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ecbb8da-087b-4fb5-a3c3-22c115f3c53a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "443     7\n",
            "441     5\n",
            "66      9\n",
            "773     5\n",
            "131     5\n",
            "       ..\n",
            "73     13\n",
            "400     9\n",
            "118     5\n",
            "701     6\n",
            "206     5\n",
            "Name: encoded_ICD10, Length: 660, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EMRDiagcdDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        # print(len(self.df))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        record = self.df.iloc[index]['transstr']\n",
        "        # diagcd = self.df.iloc[index]['ICD10']\n",
        "        diagcd = self.df.iloc[index]['encoded_ICD10']\n",
        "        # print(f\"diagcd : {diagcd} , index : {index}\")\n",
        "\n",
        "        return record, diagcd\n",
        "\n",
        "        # tokens = self.tokenizer(\n",
        "        #     record,\n",
        "        #     return_tensors='pt',\n",
        "        #     truncation=True,\n",
        "        #     padding='max_length')\n",
        "\n",
        "        # input_ids = tokens[input_ids].squeeze(0)\n",
        "        # attention_mask  = tokens[attention_mask ].squeeze(0)\n",
        "        # token_type_ids  = torch.zeros_like(token_type_ids )\n",
        "\n",
        "        # return {\n",
        "        #     'input_ids' : input_ids,\n",
        "        #     'attention_mask' : attention_mask,\n",
        "        #     'token_type_ids' : token_type_ids,\n",
        "        # }, torch.tensor(diagcd)"
      ],
      "metadata": {
        "id": "3oYtGKnFYDhy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.concat([X_train, y_train_encoding, y_train], axis=1)\n",
        "train_dataset = EMRDiagcdDataset(train_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "jHwgAYzlYQ1d"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.__getitem__(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ngL2abep2pR",
        "outputId": "58894641-0e0e-4d41-8c88-a02bd8844bfb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2021.09 CABG surgery 2023.06 CR 4.0, EGFR 14 2023.09 CR 5.4, EGFR 11, K 6.5 Seoul Samsung Hospital professor said that',\n",
              " 7)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df[:16])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNmrn2Bkf-0z",
        "outputId": "ba1cbb1a-c6f8-442a-dc58-a7e65611ca62"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              transstr  encoded_ICD10 ICD10\n",
            "443  2021.09 CABG surgery 2023.06 CR 4.0, EGFR 14 2...              7  N185\n",
            "441  Since 2016, Hematuria had hematuria in 2022 TY...              5  N183\n",
            "66   23.04 LMC obstetrics and gynecology visits Hem...              9  R311\n",
            "773  60s HTN Diagnosis 2023.11.21 CR 1.58 BP 121/67...              5  N183\n",
            "131  I didn't have a previous checkup.2023. CR 1.83...              5  N183\n",
            "785  It has been edema since childhood, and 2023.11...             10  R601\n",
            "356  # HTN (20Y)2023. CR 2.26 EGFR 20.66 HBA1C 7.3 ...              6  N184\n",
            "208  Ginseng and antler have been taking two years ...              3  N178\n",
            "346  In high school examination, Hematururia, prote...              8  N391\n",
            "540  2019. CR 1.11 EGFR 63 2022. CR 1.23 EGFR 54 20...              6  N184\n",
            "296  10 years ago, I was diagnosed with renal cyst ...             10  R601\n",
            "614  Hemodialysis was held from 2023.09.27, and it ...              7  N185\n",
            "306  I visited Postinflammation sequelae in the rig...              4  N182\n",
            "279  HTN over 5 years DM 2 years IHD controls to SG...              5  N183\n",
            "266  143/72/59 symptoms (-), GH (-) Prev.Healthy Kn...              0  N028\n",
            "153  As a spouse, I visit for the KIDNEY DONATION.A...             13  Z524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_excel('/content/train_df.xlsx')"
      ],
      "metadata": {
        "id": "qt09Opa6nu50"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam 옵티마이저 초기화\n",
        "# Adam : 신경망 훈련하는데 사용되는 최적화 알고리즘\n",
        "optimizer = Adam(model.parameters(), lr=1e-2)\n",
        "# 최적화될 모델의 매개변수(가중치와 편향)를 제공, 학습률(0.01(과학적 표기법에서 1e-2)) 설정\n",
        "criterion = torch.nn.Softmax() # Softmax 활성화 함수 생성\n",
        "itr = 1 # 반복 횟수\n",
        "p_itr = 50\n",
        "epochs = 1\n",
        "total_loss = 0 # 훈련 중 계산된 손실 저장 변수\n",
        "total_len = 0 # 훈련 중 처리된 전체 샘플 수 저장 변수\n",
        "total_correct = 0 # 분류 작업에서 훈련 중 올바르게 예측된 전체 샘플 수를 저장하는 변수"
      ],
      "metadata": {
        "id": "rBte9blF10bf"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of output classes: BertConfig: {model.config}\")"
      ],
      "metadata": {
        "id": "keTu0LlGG6ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of output classes in the model configuration\n",
        "print(f\"Number of output classes: {model.config.num_labels}\")"
      ],
      "metadata": {
        "id": "GFdLjs0fG03U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "model.train() # 모델을 훈련 모드로 설정\n",
        "\n",
        "i = 1\n",
        "\n",
        "for record, diagcd in train_loader:\n",
        "    optimizer.zero_grad() # optimizer의 gradients를 초기화\n",
        "    # print(\"record: \", record)\n",
        "    # print(\"diagcd: \", diagcd)\n",
        "    encoded_record = tokenizer(record, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    # encoded_record = encoded_record.to(device)\n",
        "    # print(type(encoded_record))\n",
        "    diagcd = torch.tensor(diagcd)\n",
        "    # diagcd = torch.tensor(list(diagcd), dim=0).to(device)\n",
        "\n",
        "    outputs = model(**encoded_record, labels=diagcd)\n",
        "\n",
        "    loss = outputs.loss\n",
        "    logits = outputs.logits\n",
        "\n",
        "    predict = torch.argmax(F.softmax(logits), dim=14)\n",
        "    correct = predict.eq(diagcd)\n",
        "\n",
        "    total_correct += correct.sum().item()\n",
        "    total_len += len(diagcd)\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if itr % p_itr == 0:\n",
        "        print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch+1, epochs, itr, total_loss/p_itr, total_correct/total_len))\n",
        "        total_loss = 0\n",
        "        total_len = 0\n",
        "        total_correct = 0\n",
        "\n",
        "    itr += 1\n",
        "\n",
        "    print(f\"i : {i}\")\n",
        "    i = i + 1\n",
        "    if (i==2) :\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "60Uyuki-AjNG",
        "outputId": "68721b5e-d67d-44e1-9adf-252864139c2e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-9daaec74e2db>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  diagcd = torch.tensor(diagcd)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "Target 7 is out of bounds.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-9daaec74e2db>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# diagcd = torch.tensor(list(diagcd), dim=0).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoded_record\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiagcd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1572\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"single_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1574\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1575\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3058\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3059\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target 7 is out of bounds."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first few elements of 'diagcd' and 'record'\n",
        "print(f\"First few labels: {diagcd[:5]}\")\n",
        "print(f\"First few records: {record[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jIYYnvqFbaN",
        "outputId": "52722c95-5431-466c-965c-ed0c797e2298"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few labels: tensor([11,  9,  5,  7,  9])\n",
            "First few records: ('The patient is a patient of 63 years old, currently being treated with the diagnostic name [unruptured aneurysm, basilar].# R/O LT.Va Dissection 2023.07.10.DSA W/U was performed herein and was diagnosed with the above findings and will be implemented in this department as a follow -up test, 07.10.LAB phase Bun/CR 28.2/1.24 (MDRD EGFR: 58.9) and confirm it and please contact us for the contrast agent CT.Thank you ns prof4 ======================================================= 143/81-67 3-4 years ago, I tested that the urine test was also okay.', 'A patient who takes various medicines but does not know about the diagnosis name, and is commissioned by the medical treatment of the Microscopic Hematuria.2022.03.08 CR 1.0 EGFR 58 Proteinuria (-) 2023.01.20 CR 0.9 EGFR 65 Proteinuria (-) Urinary SX', '157-89-72 High blood pressure and no hepatitis for 30 years (Viriad) Gout 10 years in elementary school-diagnosed with kidney disease due to hematuria.Dominated with Creainine 2.0 ~ 2.3', 'HTN, Type 2 DM Diagnosis 2023.10.16 CR 4.63, EGFR 10 BP 149/56/68', 'Browno Honorary Hematology was diagnosed since the first year of elementary school 2017, and after the pediatrics of the pediatrician, the brown rice hematuria is not specific.It was difficult to proceed herein there was no history of naked hematuria.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "model.train() # 모델을 훈련 모드로 설정\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for record, diagcd in train_loader:\n",
        "        optimizer.zero_grad() # optimizer의 gradients를 초기화\n",
        "        # print(\"record: \", record)\n",
        "        # print(\"diagcd: \", diagcd)\n",
        "        encoded_record = tokenizer(record, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        # encoded_record = encoded_record.to(device)\n",
        "        diagcd = diagcd.clone().detach()\n",
        "        # print(type(encoded_record))\n",
        "        # diagcd = torch.tensor(diagcd)\n",
        "        # diagcd = torch.tensor(list(diagcd), dim=0).to(device)\n",
        "\n",
        "        outputs = model(**encoded_record, labels=diagcd)\n",
        "\n",
        "        loss = outputs.loss # 모델의 출력 손실\n",
        "        logits = outputs.logits # 모델의 출력 로짓(확률을 계산하기 전의 값)\n",
        "\n",
        "        predict = torch.argmax(F.softmax(logits), dim=14)\n",
        "        correct = predict.eq(diagcd)\n",
        "\n",
        "        total_correct += correct.sum().item()\n",
        "        total_len += len(diagcd)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if itr % p_itr == 0:\n",
        "            # print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch+1, epochs, itr, total_loss/p_itr, total_correct/total_len))\n",
        "            total_loss = 0\n",
        "            total_len = 0\n",
        "            total_correct = 0\n",
        "\n",
        "        itr += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "TiX8sQaST6S8",
        "outputId": "af609deb-d1a1-4a81-a5a6-c098ee87db0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-23-4d0f789a0c84>, line 5)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-4d0f789a0c84>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    for record, diagcd in train_loader:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.concat([X_test, y_test], axis=1)\n",
        "print(test_df)\n",
        "\n",
        "test_dataset = EMRDiagcdDataset(test_df)\n",
        "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True)"
      ],
      "metadata": {
        "id": "rNjToVkPYNAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation\n",
        "model.eval()\n",
        "\n",
        "total_len = 0\n",
        "total_correct = 0\n",
        "\n",
        "for record, diagcd in test_loader:\n",
        "\n",
        "    encoded_record = tokenizer(record, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    encoded_record, diagcd = encoded_record.to(device), diagcd.to(device)\n",
        "\n",
        "    outputs = model(**encoded_record, labels=diagcd)\n",
        "\n",
        "    logits = outputs.logits\n",
        "\n",
        "    predict = torch.argmax(F.softmax(logits), dim=14)\n",
        "    correct = predict.eq(diagcd)\n",
        "\n",
        "    total_correct += correct.sum().item()\n",
        "    total_len += len(diagcd)\n",
        "\n",
        "print('Test accuracy: ', total_correct / total_len)"
      ],
      "metadata": {
        "id": "nOFsEw-xXmg5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}